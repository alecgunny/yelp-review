{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27652087-a815-4934-b267-a27efef3611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datasets\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.palettes import Bright5 as palette\n",
    "from bokeh.plotting import figure\n",
    "from lightning import pytorch as pl\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    format=\"%(asctime)s [%(name)s] - %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "pl.seed_everything(101588, workers=True)\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "class ZipDataset:\n",
    "    def __init__(self, **tensors):\n",
    "        tensors = {k: torch.from_numpy(v) for k, v in tensors.items()}\n",
    "        self.datasets = {\n",
    "            k: torch.utils.data.TensorDataset(v) for k, v in tensors.items()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(list(self.datasets.values())[0])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {k: v[i] for k, v in self.datasets.items()}\n",
    "\n",
    "\n",
    "class Dataset(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        dataset: str,\n",
    "        batch_size: int,\n",
    "        subset_size: int | None = None,\n",
    "        force: bool = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.datasets = {}\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize(self, examples):\n",
    "        return self.tokenizer(examples, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        if os.path.exists(self.hparams.dataset) and not self.hparams.force:\n",
    "            with h5py.File(self.hparams.dataset, \"r\") as f:\n",
    "                if f.attrs[\"model_name\"] != self.hparams.model_name:\n",
    "                    raise ValueError(\n",
    "                        \"Dataset {} was tokenized with model {}, \"\n",
    "                        \"doesn't match specified model {}\".format(\n",
    "                            self.hparams.dataset,\n",
    "                            f.attrs[\"model_name\"],\n",
    "                            self.hparams.model_name\n",
    "                        )\n",
    "                    )\n",
    "            logging.info(\"Dataset already exists and not forcing overwrite\")\n",
    "            return\n",
    "        elif self.hparams.force:\n",
    "            logging.info(\"Dataset already exists but forcing overwrite\")\n",
    "\n",
    "        logging.info(\"Downloading and preprocessing dataset\")\n",
    "        dataset = datasets.load_dataset(\"yelp_review_full\")\n",
    "        dataset = dataset.map(lambda x: self.tokenize(x[\"text\"]), batched=True)\n",
    "        dataset.set_format(\"numpy\")\n",
    "\n",
    "        logging.info(f\"Writing data to {self.hparams.dataset}\")\n",
    "        with h5py.File(self.hparams.dataset, \"w\") as f:\n",
    "            f.attrs[\"model_name\"] = self.hparams.model_name\n",
    "\n",
    "            for split, dset in dataset.items():\n",
    "                group = f.create_group(split)\n",
    "                for feature in dset.features:\n",
    "                    X = dset[feature]\n",
    "                    if feature == \"text\":\n",
    "                        group.attrs[\"text\"] = \"\\n\".join(X)\n",
    "                    else:\n",
    "                        chunks = (1000,)\n",
    "                        if X.ndim == 2:\n",
    "                            chunks = chunks + (X.shape[-1],)\n",
    "                        group.create_dataset(feature, data=X, chunks=chunks)\n",
    "                    f.flush()\n",
    "        del dataset\n",
    "\n",
    "    def setup(self, stage):\n",
    "        split = \"test\" if stage != \"fit\" else \"train\"\n",
    "\n",
    "        logging.info(f\"Loading {split} data\")\n",
    "        with h5py.File(self.hparams.dataset, \"r\") as f:\n",
    "            if split == \"train\" and self.hparams.subset_size is not None:\n",
    "                dataset = f[split]\n",
    "                size = len(list(dataset.values())[0])\n",
    "                idx = np.random.choice(\n",
    "                    size, size=self.hparams.subset_size, replace=False\n",
    "                )\n",
    "                idx = np.sort(idx)\n",
    "            else:\n",
    "                idx = slice(None, None)\n",
    "\n",
    "            self.datasets[split] = {k: v[idx] for k, v in f[split].items()}\n",
    "\n",
    "        labels = self.datasets[split].pop(\"label\")\n",
    "        labels = labels / 2 - 1\n",
    "        self.datasets[split][\"labels\"] = labels\n",
    "\n",
    "    def on_after_batch_transfer(self, batch, _):\n",
    "        converted = {}\n",
    "        for k, v in batch.items():\n",
    "            v = v[0]\n",
    "            if k != \"labels\":\n",
    "                v = v.type(torch.int64)\n",
    "            converted[k] = v\n",
    "        return converted\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = ZipDataset(**self.datasets[\"train\"])\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            shuffle=True,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        dataset = ZipDataset(**self.datasets[\"test\"])\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            shuffle=False,\n",
    "            batch_size=4 * self.hparams.batch_size,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, model_name: str, learning_rate: float = 1e-3) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=1\n",
    "        )\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = X.pop(\"labels\").type(torch.float32)\n",
    "        return self.model(**X).logits[:, 0], y\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss = self.loss(*self(batch))\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = self.model.parameters()\n",
    "        optimizer = torch.optim.AdamW(params, self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            self.hparams.learning_rate,\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "            pct_start=0.1\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def configure_callbacks(self) -> list[pl.Callback]:\n",
    "        checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "            filename=\"weights.pt\",\n",
    "            save_last=True,\n",
    "            auto_insert_metric_name=False,\n",
    "        )\n",
    "        return [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e45acb-55e5-4c97-ba8d-6d07a813cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"dataset.hdf5\"\n",
    "model_name = \"bert-base-cased\"\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "max_epochs = 1\n",
    "subset_size = 200000\n",
    "\n",
    "logger = pl.loggers.CSVLogger(\"logs\", name=model_name)\n",
    "dataset = Dataset(model_name, dataset, batch_size, subset_size)\n",
    "model = Model(model_name, learning_rate)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    devices=[0],\n",
    "    precision=\"16-mixed\",\n",
    "    accumulate_grad_batches=8,\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ed0e4-7547-4bc5-96fd-37a48a73c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51185d5c-19f9-4fc5-afe2-72d21feaa256",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(model, dataset)\n",
    "\n",
    "y_hat = torch.cat([i[0] for i in predictions]).cpu().numpy()\n",
    "y = torch.cat([i[1] for i in predictions]).cpu().numpy()\n",
    "df = pd.DataFrame(dict(predictions=y_hat, labels=y))\n",
    "df.to_csv(os.path.join(logger.log_dir, \"results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c622dd8e-0769-48fe-af09-1526c4220d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(os.path.join(logger.log_dir, \"metrics.csv\"))\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02cd1b-9ed2-4401-9fdc-48dc56991b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(\n",
    "    height=300,\n",
    "    width=700,\n",
    "    x_axis_label=\"Step\",\n",
    "    y_axis_label=\"Train loss\",\n",
    "    tools=\"\",\n",
    "    tooltips=[(\"Step\", \"@step\"), (\"Loss\", \"@train_loss_step\")]\n",
    ")\n",
    "p.line(\n",
    "    \"step\",\n",
    "    \"train_loss_step\",\n",
    "    line_color=palette[0],\n",
    "    line_width=1.5,\n",
    "    line_alpha=0.9,\n",
    "    source=metrics\n",
    ")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff75725-823b-4337-b4d5-64e80912bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(os.path.join(logger.log_dir, \"results.csv\"))\n",
    "slope, _ = np.polyfit(results.predictions, results.labels, 1)\n",
    "label_variance = results.labels.var()\n",
    "residuals = (results.diff(axis=1)**2).mean().labels\n",
    "r2 = 1 - residuals / label_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d808a8-bb2d-49f9-b641-f09e166d92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(\n",
    "    title=rf\"$$\\text{{Slope: {slope:0.3f}, R}}^2\\text{{: {r2:0.4f}}}$$\",\n",
    "    height=300,\n",
    "    width=700,\n",
    "    x_axis_label=\"Predicted Score\",\n",
    "    y_axis_label=\"Density\"\n",
    ")\n",
    "p.title.text_font_style = \"normal\"\n",
    "p.toolbar_location = None\n",
    "\n",
    "for i, (label, df) in enumerate(results.groupby(\"labels\")):\n",
    "    hist, bins = np.histogram(df.predictions, bins=50)\n",
    "    hist = hist / hist.sum() / (bins[1] - bins[0])\n",
    "    centers = (bins[:1] + bins[:-1]) / 2\n",
    "\n",
    "    label = (label + 1) * 2\n",
    "    p.line(\n",
    "        (centers + 1) * 2,\n",
    "        hist,\n",
    "        line_color=palette[i],\n",
    "        line_width=2,\n",
    "        line_alpha=0.8,\n",
    "        legend_label=str(int(label))\n",
    "    )\n",
    "p.legend.title = \"True Label\"\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
